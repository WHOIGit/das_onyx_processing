{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Routines to create decimated DAS datasets\n",
    "- read h5 into xarray\n",
    "- concatenate 1-minute files into longer (e.g., 30-min) chunks\n",
    "- filter w butterworth for wave frequencies\n",
    "- decimate (to 5 Hz)\n",
    "- save 30-min decimated to netcdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import h5py as h5\n",
    "from matplotlib import pyplot as plt\n",
    "from datetime import date, datetime\n",
    "import xarray as xr\n",
    "import glob\n",
    "from scipy import signal\n",
    "from joblib import Parallel, delayed\n",
    "import time as tm\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datetime_range(start, end, delta):\n",
    "    current = start\n",
    "    while current < end:\n",
    "        yield current\n",
    "        current += delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_h5_into_xr_chunk(base_path, time_target, time_chunk):\n",
    "    #file names for all files including times within the 30 min chunk\n",
    "    files_target_chunk = [ glob.glob(onyx_path + '*' + datetime.strftime(time_temp,'%Y-%m-%d_%H.%M') +'*.h5') for time_temp in \n",
    "           datetime_range(time_target-timedelta(minutes=1), time_target + timedelta(minutes=time_chunk), \n",
    "           timedelta(minutes=1))]\n",
    "    for fi in files_target_chunk:\n",
    "        #fi = files_target_chunk[0]\n",
    "        if not fi:\n",
    "            print(\"List is empty\")\n",
    "            continue\n",
    "        else: #only continue if there is a fule \n",
    "                \n",
    "            f = h5.File(fi[0],'r')\n",
    "\n",
    "            #data and key parameters\n",
    "            f_data = np.array(f['Acquisition']['Raw[0]']['RawData'])\n",
    "            f_sampcount = np.array(f['Acquisition']['Raw[0]']['RawDataSampleCount'])\n",
    "            channels = np.arange(0,f_data.shape[1])*f['Acquisition'].attrs['SpatialSamplingInterval']\n",
    "\n",
    "            #create actual times, where file time is in microseconds from #\n",
    "            file_timestr = fi[0].split('/')[-1][10:-7]\n",
    "            #print(file_timestr)\n",
    "            file_datetime = datetime.strptime(file_timestr, '%Y-%m-%d_%H.%M.%S')\n",
    "            f_seconds = f_data.shape[0]/f['Acquisition'].attrs['PulseRate'] #length of time, in seconds, of array\n",
    "            dt_ms = 1000000/f['Acquisition'].attrs['PulseRate'] #time in ms between each ...at 250 Hz, 4000 micros between each timestep\n",
    "\n",
    "            f_time = [dt for dt in \n",
    "                   datetime_range(file_datetime, file_datetime + timedelta(seconds=f_seconds), \n",
    "                   timedelta(microseconds=dt_ms))]\n",
    "\n",
    "            data_DAS = {'strain':(['time','channels'], f_data, \n",
    "                                {'units':'',\n",
    "                               'long_name':'strain data'})}\n",
    "\n",
    "            # define coordinates\n",
    "            coords = {'time': (['time'], f_time),\n",
    "                      'channels': (['channels'], channels)}\n",
    "            #define attributes, all from hdf5 file\n",
    "            attrs = dict()\n",
    "            for fi,fi_attr in enumerate(f['Acquisition'].attrs.keys()):\n",
    "                if isinstance(f['Acquisition'].attrs[fi_attr], bytes):\n",
    "                    attrs[fi_attr] = f['Acquisition'].attrs[fi_attr].decode(\"utf-8\")\n",
    "                else:\n",
    "                    attrs[fi_attr] = f['Acquisition'].attrs[fi_attr] \n",
    "\n",
    "            #create dataset\n",
    "            ds_DAS = xr.Dataset(data_vars=data_DAS, \n",
    "                            coords=coords)\n",
    "\n",
    "            if 'ds_DAS_chunk' in locals():\n",
    "                ds_DAS_chunk = xr.merge([ds_DAS_chunk,ds_DAS])\n",
    "            else:\n",
    "                ds_DAS_chunk = ds_DAS\n",
    "    ds_DAS_chunk = ds_DAS_chunk.assign_attrs(attrs)\n",
    "    \n",
    "    #select exactly the 30 minutes from the full combined array\n",
    "    fs = ds_DAS_chunk.attrs['PulseRate']\n",
    "    ds_DAS_chunk = ds_DAS_chunk.sel(time=slice(time_target, time_target+timedelta(minutes=time_chunk)))\n",
    "    if len(ds_DAS_chunk.time) < time_chunk*fs*60:\n",
    "        print('Stop, missing data: '+str(len(ds_DAS_chunk.time)) + ' should be ' + str(time_chunk*ds_DAS_chunk.attrs['PulseRate']*60)) \n",
    "\n",
    "    return ds_DAS_chunk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"        f_seconds = f_data.shape[0]/f['Acquisition'].attrs['PulseRate'] #length of time, in seconds, of array\n",
    "        dt_ms = 1000000/f['Acquisition'].attrs['PulseRate'] #time in ms between each ...at 250 Hz, 4000 micros between each timestep\n",
    "        \n",
    "        f_time = [dt for dt in \n",
    "               datetime_range(file_datetime, file_datetime + timedelta(seconds=f_seconds), \n",
    "               timedelta(microseconds=dt_ms))]\n",
    "               \n",
    "               \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def das_butterworth_decimate_xarray(ds_DAS_chunk, fs_target):\n",
    "    \n",
    "    #â€œt_inc\" parameter is the an integer representing the multiples you need to downsample\n",
    "    #where fs is the original sampling rate and 50 is the frequency you want\n",
    "    fs = ds_DAS_chunk.attrs['PulseRate']\n",
    "    t_inc = int(fs/fs_target) \n",
    "\n",
    "    #initialize empty nan array for decimated data\n",
    "    ds_DAS_deci = np.empty((len(ds_DAS_chunk.time[0::t_inc]),len(ds_DAS_chunk.channels)))\n",
    "    ds_DAS_deci[:] = np.nan\n",
    "\n",
    "    #butterworth filter, use for surface waves\n",
    "\n",
    "    #define butterworth filter \n",
    "    cutoff = fs_target #desire cutoff frequency of filter, Hz\n",
    "    nyq = 0.5*fs #nyquist frequency\n",
    "    order = 1\n",
    "    normal_cutoff = cutoff/nyq\n",
    "    b_butter, a_butter = signal.butter(order,normal_cutoff,btype='low',analog=False)\n",
    "\n",
    "    for i, ci in enumerate(ds_DAS_chunk.channels.values):\n",
    "        strain = ds_DAS_chunk.strain.transpose().values[i]\n",
    "        strain_butter = signal.filtfilt(b_butter, a_butter, strain)\n",
    "        strain_deci_butter = strain_butter[::t_inc]\n",
    "        ds_DAS_deci[:,i] = strain_deci_butter\n",
    "\n",
    "    #make xarray\n",
    "\n",
    "    attrs_deci = attrs\n",
    "    attrs_deci['PulseRateDecimated']=fs_target\n",
    "    attrs_deci['DecimationFilterType']='butterworth'\n",
    "\n",
    "    coords = {'time': (['time'], ds_DAS_chunk.time[0::t_inc]),\n",
    "                  'channels': (['channels'], channels)}\n",
    "\n",
    "    data_deci = {'strain':(['time','channels'], ds_DAS_deci, \n",
    "                            {'units':'',\n",
    "                           'long_name':'decimated strain data'})}\n",
    "    strain_deci_butter_all = xr.Dataset(data_vars=data_deci,coords=coords,attrs=attrs_deci)\n",
    "    \n",
    "    return strain_deci_butter_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create 5 Hz decimated dataset for waves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_chunk = 30 #min files \n",
    "fs_target = 5 #target frequency, Hz\n",
    "\n",
    "#files to read in for target time \n",
    "onyx_path = '/Volumes/OnyxDASdata/FEB_DATA/'\n",
    "\n",
    "#range of target times\n",
    "start_time = datetime(2023, 11, 23, 3, 0) #2023,11,22,0,0 having issues matching strain length to time length... \n",
    "end_time = datetime(2023, 12, 18, 0, 0)\n",
    "\n",
    "def datetime_range(start, end, delta):\n",
    "    current = start\n",
    "    while current < end:\n",
    "        yield current\n",
    "        current += delta\n",
    "#time_target = datetime(2023,11,22,0,0,0)\n",
    "#ds_DAS_chunk = load_h5_into_xr_chunk(onyx_path, time_target, time_chunk)\n",
    "\n",
    "#directory for saving decimated\n",
    "dir_5hz = '/Users/msmith/Documents/DAS/MVCO/202311_MVCO/Onyx_DASdata_5hz/'\n",
    "\n",
    "for di in datetime_range(start_time, end_time, timedelta(minutes=30)):\n",
    "    print(str(di)+', current time '+str(datetime.now()))\n",
    "    ds_DAS_chunk = load_h5_into_xr_chunk(onyx_path, di, time_chunk)\n",
    "    strain_deci_butter_all = das_butterworth_decimate_xarray(ds_DAS_chunk,fs_target)\n",
    "    \n",
    "    strain_deci_butter_all.to_netcdf(dir_5hz+'Onyx_'+datetime.strftime(di,'%Y-%m-%d_%H.%M')+'_'+str(fs_target)+'hz.nc')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO DO \n",
    "- loop through multiple folders to get all 30 min as needed (for FEB_DATA_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
